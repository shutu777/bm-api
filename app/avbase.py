"""
AVBase çˆ¬è™«å·¥å…·ï¼šæŠ“å– https://www.avbase.net/works çš„æœç´¢é¡µé¢ï¼Œåªè§£æå‰äº”ä¸ªå¡ç‰‡ï¼Œè¾“å‡ºç•ªå·ã€æ ‡é¢˜ä¸æ¼”å‘˜åˆ—è¡¨ã€‚
"""
from __future__ import annotations

import logging
import re
from typing import Iterable, List, Optional, Set
from urllib.parse import quote

import requests
from bs4 import BeautifulSoup, Tag

try:  # pragma: no cover - å¯é€‰ä¾èµ–
    import cloudscraper
except ImportError:  # pragma: no cover - å¯é€‰ä¾èµ–
    cloudscraper = None

logger = logging.getLogger("avbase")

BASE_URL = "https://www.avbase.net/works"
TIMEOUT = 15
MAX_RESULTS = 5
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/122.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "en-US,en;q=0.9",
    "Accept": (
        "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,"
        "image/webp,image/apng,*/*;q=0.8"
    ),
    "Referer": "https://www.avbase.net/works",
    "Cache-Control": "no-cache",
}


def _create_session() -> requests.Session:
    if cloudscraper is not None:
        session = cloudscraper.create_scraper(
            browser={"browser": "chrome", "platform": "windows", "mobile": False}
        )
    else:
        session = requests.Session()
    session.headers.update(HEADERS)
    return session


def _has_required_outer_classes(classes: Optional[Iterable[str]]) -> bool:
    """
    ç½‘æ ¼å®¹å™¨é€šå¸¸åŒ…å« `grid` ä¸ `gap-4`ï¼Œåªè¦æ»¡è¶³è¿™ä¸¤ä¸ªç±»åå³å¯è§†ä¸ºæœ‰æ•ˆï¼Œé¿å… Tailwind çš„åŠ¨æ€ç±»å¯¼è‡´è§£æå¤±è´¥ã€‚
    """
    if not classes:
        return False
    if isinstance(classes, str):
        classes = classes.split()
    return {"grid", "gap-4"}.issubset(set(classes))


def _normalize_actor_name(name: str) -> str:
    cleaned = name.strip().lstrip("[(").rstrip("])").strip()
    cleaned = re.sub(r"\s+", " ", cleaned)
    return cleaned


def _extract_actor_names(card: Tag) -> List[str]:
    chips = card.select("div.bg-base-100 a.chip")
    actors: List[str] = []
    for chip in chips:
        name = _normalize_actor_name(chip.get_text(strip=True))
        if name:
            actors.append(name)
    return actors


def _extract_code(card: Tag) -> str:
    code_tag = card.select_one("span.font-bold.text-gray-500")
    return code_tag.get_text(strip=True) if code_tag else "UNKNOWN"


def _extract_title(card: Tag) -> str:
    title_tag = card.select_one("a.text-md.font-bold")
    return title_tag.get_text(strip=True) if title_tag else ""


def _parse_cards(html: str) -> List[dict]:
    soup = BeautifulSoup(html, "html.parser")
    grid_container = soup.find("div", class_=_has_required_outer_classes)  # type: ignore[arg-type]
    if grid_container is None:
        logger.info("âš ï¸ AVBase é¡µé¢æœªæ‰¾åˆ°ç»“æœç½‘æ ¼ï¼Œè§†ä¸ºæ— ç»“æœ")
        return []

    cards = grid_container.find_all("div", class_="relative", limit=MAX_RESULTS)
    results = []
    for card in cards:
        results.append(
            {
                "code": _extract_code(card),
                "title": _extract_title(card),
                "actors": _extract_actor_names(card),
            }
        )
    return results


CODE_PATTERN = re.compile(r"^[A-Z0-9]{2,}-[A-Z0-9]{2,}$")
_ALNUM_CODE_PATTERN = re.compile(r"^(?=.*[A-Za-z])(?=.*\d)[A-Za-z0-9_-]+$")


def _parse_proxy_cards(markdown: str) -> List[dict]:
    lines = markdown.splitlines()
    cards: List[dict] = []
    i = 0
    while i < len(lines):
        code_candidate = lines[i].strip()
        if CODE_PATTERN.match(code_candidate):
            chunk: List[str] = []
            i += 1
            while i < len(lines):
                next_line = lines[i]
                if CODE_PATTERN.match(next_line.strip()):
                    break
                chunk.append(next_line)
                i += 1
            content = "\n".join(chunk)
            title_match = re.search(
                r"\[([^\]]+)\]\(https?://www\.avbase\.net/works/(?!date)",
                content,
            )
            actors = []
            for actor_raw in re.findall(
                r"\)\s*([^\]\n]+)\]\(https?://www\.avbase\.net/talents/", content
            ):
                cleaned = _normalize_actor_name(actor_raw)
                if cleaned:
                    actors.append(cleaned)
            cards.append(
                {
                    "code": code_candidate,
                    "title": title_match.group(1).strip() if title_match else "",
                    "actors": actors,
                }
            )
            if len(cards) >= MAX_RESULTS:
                break
        else:
            i += 1
    return cards


def _fetch_via_proxy(keyword: str) -> str:
    proxy_url = (
        "https://r.jina.ai/http://www.avbase.net/works?q=" + quote(keyword, safe="")
    )
    response = requests.get(proxy_url, headers=HEADERS, timeout=TIMEOUT)
    response.raise_for_status()
    return response.text


def _fetch_direct_html(keyword: str) -> str:
    keyword = (keyword or "").strip()
    if not keyword:
        return ""

    session = _create_session()
    # æœªå®‰è£… Cloudscraper æ—¶ï¼Œå…ˆè®¿é—®ä¸€æ¬¡åŸºç¡€é¡µé¢ä»¥è·å–å¿…è¦ Cookieã€‚
    if cloudscraper is None:
        try:
            session.get(BASE_URL, timeout=TIMEOUT)
        except requests.RequestException as exc:
            logger.debug("âš ï¸ åˆå§‹åŒ– AVBase ä¼šè¯å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œï¼š%s", exc)

    response = session.get(
        BASE_URL,
        params={"q": keyword},
        timeout=TIMEOUT,
    )
    response.raise_for_status()
    logger.info("âœ… å·²å®Œæˆ AVBase æŠ“å–ï¼Œå…³é”®å­—=%s", keyword)
    return response.text


def is_code_like(keyword: str) -> bool:
    """
    åˆ¤æ–­å…³é”®å­—æ˜¯å¦ç±»ä¼¼ç•ªå·ï¼ˆå¿…é¡»åŒ…å«å­—æ¯ä¸æ•°å­—ï¼Œå¯å«æ¨ªçº¿å’Œä¸‹åˆ’çº¿ï¼Œå¿…é¡»æ˜¯ ASCIIï¼‰ã€‚
    """
    if not keyword:
        return False
    stripped = "".join(ch for ch in keyword if not ch.isspace())
    if not stripped:
        return False
    return bool(_ALNUM_CODE_PATTERN.match(stripped))


def filter_actor_cards(cards: List[dict], keyword: str) -> List[dict]:
    """
    å…³é”®å­—åŒ¹é…æ¼”å‘˜å§“åï¼Œåªä¿ç•™å‘½ä¸­çš„æ¼”å‘˜ä¸å¡ç‰‡ã€‚
    """
    needle = keyword.strip().lower()
    if not needle:
        return cards
    needle_simple = needle.replace(" ", "")
    filtered_cards: List[dict] = []
    for card in cards:
        names = card.get("actors", [])
        matched = []
        for name in names:
            normalized = name.lower()
            normalized_simple = normalized.replace(" ", "")
            if needle in normalized or (needle_simple and needle_simple in normalized_simple):
                matched.append(name)
        if matched:
            filtered_cards.append({**card, "actors": matched})
    return filtered_cards


def collapse_actor_list(cards: List[dict], limit: int = MAX_RESULTS) -> List[str]:
    """
    å°†æ¼”å‘˜åˆ—è¡¨æ‰å¹³åŒ–å¹¶å»é‡ï¼Œæ¯å¼ å¡ç‰‡åªå–ç¬¬ä¸€ä¸ªæ¼”å‘˜ï¼Œé™åˆ¶æœ€å¤š `limit` ä¸ªã€‚
    """
    unique: Set[str] = set()
    collapsed: List[str] = []
    for card in cards:
        actors = card.get("actors") or []
        if not actors:
            continue
        first_actor = actors[0]
        normalized = first_actor.strip()
        if not normalized:
            continue
        lower = normalized.lower()
        if lower in unique:
            continue
        unique.add(lower)
        collapsed.append(normalized)
        if len(collapsed) >= limit:
            break
    return collapsed


def search_avbase(keyword: str) -> List[dict]:
    """
    æ ¹æ®å…³é”®å­—æŠ“å– AVBaseï¼Œä¼˜å…ˆä½¿ç”¨é•œåƒé¿å… Cloudflare æ‹¦æˆªã€‚
    """
    keyword = (keyword or "").strip()
    if not keyword:
        return []

    try:
        markdown = _fetch_via_proxy(keyword)
        proxy_cards = _parse_proxy_cards(markdown)
        if proxy_cards:
            logger.info("ğŸš€ é€šè¿‡é•œåƒæŠ“å–åˆ° %s æ¡ AVBase ç»“æœï¼š%s", len(proxy_cards), keyword)
            return proxy_cards
    except requests.RequestException as exc:
        logger.warning("âš ï¸ é•œåƒæŠ“å–å¤±è´¥ï¼Œå›é€€ä¸ºç›´è¿ï¼š%s", exc)

    try:
        html = _fetch_direct_html(keyword)
        if not html:
            return []
        return _parse_cards(html)
    except requests.HTTPError as exc:
        status = exc.response.status_code if exc.response is not None else None
        if status == 403:
            logger.warning("âŒ ç›´è¿ AVBase ä»è¢« 403 æ‹¦æˆªï¼Œè¿”å›ç©ºç»“æœï¼š%s", keyword)
            return []
        raise


__all__ = ["search_avbase", "is_code_like", "filter_actor_cards", "collapse_actor_list"]
